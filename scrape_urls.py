#!/usr/bin/env python3
"""
Script para scrapear URLs individuales de Idealista y aÃ±adirlas a Google Sheets
Uso: python scrape_urls.py urls.txt
"""

import sys
from idealista_scraper import IdealistaScraper
from google_sheets import GoogleSheetsManager


def scrape_urls_from_file(filename):
    """Lee URLs de un archivo y las scrapea"""
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                            â•‘
â•‘        ğŸ•·ï¸  SCRAPER DE URLs DE IDEALISTA  ğŸ•·ï¸               â•‘
â•‘                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # Leer URLs del archivo
    try:
        with open(filename, 'r') as f:
            urls = [line.strip() for line in f if line.strip() and line.strip().startswith('http')]
    except FileNotFoundError:
        print(f"âŒ No se encontrÃ³ el archivo: {filename}")
        print("\nCrea un archivo con una URL por lÃ­nea:")
        print("  https://www.idealista.com/inmueble/108542671/")
        print("  https://www.idealista.com/inmueble/108542672/")
        return
    
    if not urls:
        print("âŒ No se encontraron URLs vÃ¡lidas en el archivo")
        return
    
    print(f"ğŸ“‹ Se encontraron {len(urls)} URLs para scrapear\n")
    
    # Inicializar scraper y Google Sheets
    scraper = IdealistaScraper()
    sheets = GoogleSheetsManager()
    sheets.get_or_create_spreadsheet()
    
    print(f"ğŸ“Š Google Sheets: {sheets.get_spreadsheet_url()}\n")
    print("="*60)
    
    # Scrapear cada URL
    success_count = 0
    failed_count = 0
    
    for i, url in enumerate(urls, 1):
        print(f"\n[{i}/{len(urls)}] Procesando...")
        
        try:
            # Scrapear la propiedad
            property_data = scraper.scrape_property_url(url)
            
            if property_data:
                # Verificar si ya existe
                property_id = property_data['id']
                if sheets.property_exists(property_id):
                    print(f"âš ï¸  La propiedad {property_id} ya existe en la hoja")
                else:
                    # AÃ±adir a Google Sheets
                    if sheets.add_property(property_data):
                        success_count += 1
                        print(f"âœ… AÃ±adida: {property_data['titulo'][:50]}...")
                        print(f"   ğŸ’° {property_data['precio']:,.0f}â‚¬ | ğŸ“ {property_data['tamaÃ±o']}mÂ²")
                    else:
                        failed_count += 1
            else:
                failed_count += 1
            
            # Delay entre peticiones para ser respetuoso con el servidor
            if i < len(urls):
                print("â³ Esperando 2 segundos...")
                scraper.delay_between_requests(2)
            
        except Exception as e:
            print(f"âŒ Error procesando URL: {e}")
            failed_count += 1
    
    # Resumen
    print("\n" + "="*60)
    print("ğŸ“Š RESUMEN")
    print("="*60)
    print(f"âœ… Propiedades aÃ±adidas: {success_count}")
    print(f"âŒ Errores: {failed_count}")
    print(f"ğŸ“Š Total procesadas: {len(urls)}")
    print(f"\nğŸ”— Ver en Google Sheets: {sheets.get_spreadsheet_url()}")


def scrape_single_url(url):
    """Scrapea una sola URL y la aÃ±ade a Google Sheets"""
    
    print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                            â•‘
â•‘        ğŸ•·ï¸  SCRAPER DE URL DE IDEALISTA  ğŸ•·ï¸                â•‘
â•‘                                                            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    print(f"ğŸ” URL: {url}\n")
    
    # Inicializar scraper y Google Sheets
    scraper = IdealistaScraper()
    sheets = GoogleSheetsManager()
    sheets.get_or_create_spreadsheet()
    
    print(f"ğŸ“Š Google Sheets: {sheets.get_spreadsheet_url()}\n")
    print("="*60 + "\n")
    
    # Scrapear la propiedad
    property_data = scraper.scrape_property_url(url)
    
    if property_data:
        # Mostrar datos extraÃ­dos
        print("\nğŸ“‹ Datos extraÃ­dos:")
        print(f"  ID: {property_data['id']}")
        print(f"  TÃ­tulo: {property_data['titulo']}")
        print(f"  Precio: {property_data['precio']:,.0f}â‚¬")
        print(f"  TamaÃ±o: {property_data['tamaÃ±o']}mÂ²")
        print(f"  Precio/mÂ²: {property_data['precio_m2']:.0f}â‚¬/mÂ²")
        print(f"  Habitaciones: {property_data['habitaciones']}")
        print(f"  BaÃ±os: {property_data['baÃ±os']}")
        print(f"  UbicaciÃ³n: {property_data['direccion']}")
        
        # Verificar si ya existe
        if sheets.property_exists(property_data['id']):
            print(f"\nâš ï¸  La propiedad {property_data['id']} ya existe en la hoja")
        else:
            # AÃ±adir a Google Sheets
            if sheets.add_property(property_data):
                print(f"\nâœ… Â¡Propiedad aÃ±adida exitosamente a Google Sheets!")
                print(f"ğŸ”— {sheets.get_spreadsheet_url()}")
            else:
                print(f"\nâŒ Error aÃ±adiendo la propiedad a Google Sheets")
    else:
        print("\nâŒ No se pudieron extraer los datos de la propiedad")


def main():
    """FunciÃ³n principal"""
    
    if len(sys.argv) < 2:
        print("Uso:")
        print("  python scrape_urls.py <archivo_urls.txt>    # MÃºltiples URLs")
        print("  python scrape_urls.py <url>                  # Una sola URL")
        print()
        print("Ejemplos:")
        print("  python scrape_urls.py urls.txt")
        print("  python scrape_urls.py https://www.idealista.com/inmueble/108542671/")
        return
    
    arg = sys.argv[1]
    
    # Detectar si es una URL o un archivo
    if arg.startswith('http'):
        scrape_single_url(arg)
    else:
        scrape_urls_from_file(arg)


if __name__ == '__main__':
    main()


