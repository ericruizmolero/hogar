# ğŸ•·ï¸ GuÃ­a de Scraping de Idealista

## Â¿Por quÃ© Scraping?

### âœ… Ventajas ENORMES

1. **GRATIS al 100%** - No necesitas ninguna API ni credenciales
2. **Sin aprobaciones** - Empieza inmediatamente
3. **Sin lÃ­mites** - No hay cuotas de peticiones
4. **MÃ¡s datos** - Acceso a TODA la informaciÃ³n visible
5. **MÃ¡s simple** - No OAuth, no tokens, solo HTTP

### âš ï¸ Consideraciones

1. **Legalidad**: Revisa los tÃ©rminos de servicio de Idealista
2. **Rate limiting**: Respeta el servidor (delays entre peticiones)
3. **Mantenimiento**: Si cambian el HTML, hay que adaptar
4. **Ã‰tica**: No satures el servidor, sÃ© respetuoso

## ğŸ¯ Tres Formas de Usar el Scraper

### 1ï¸âƒ£ Scrapear URLs Individuales (Manual)

**Ideal para**: Propiedades especÃ­ficas que ya conoces

```bash
# Una sola URL
python scrape_urls.py "https://www.idealista.com/inmueble/108542671/"

# MÃºltiples URLs desde un archivo
python scrape_urls.py urls.txt
```

**Crear archivo urls.txt**:
```txt
https://www.idealista.com/inmueble/108542671/
https://www.idealista.com/inmueble/108542672/
https://www.idealista.com/inmueble/108542673/
```

### 2ï¸âƒ£ Tracker AutomÃ¡tico (Recomendado)

**Ideal para**: Monitoreo continuo de una bÃºsqueda

```bash
# Modo continuo (revisa cada X minutos)
python scrape_tracker.py "https://www.idealista.com/venta-viviendas/madrid/chamberi/"

# BÃºsqueda Ãºnica
python scrape_tracker.py "https://www.idealista.com/venta-viviendas/madrid/" once
```

### 3ï¸âƒ£ Scraping Avanzado (ProgramÃ¡tico)

**Ideal para**: IntegraciÃ³n personalizada

```python
from idealista_scraper import IdealistaScraper
from google_sheets import GoogleSheetsManager

scraper = IdealistaScraper()
sheets = GoogleSheetsManager()
sheets.get_or_create_spreadsheet()

# Scrapear una URL
data = scraper.scrape_property_url("https://www.idealista.com/inmueble/108542671/")
sheets.add_property(data)
```

## ğŸ“– GuÃ­a Paso a Paso: Tracker AutomÃ¡tico

### Paso 1: Obtener URL de BÃºsqueda

1. Ve a **idealista.com**
2. Realiza tu bÃºsqueda con filtros:
   - UbicaciÃ³n (ej: Madrid, ChamberÃ­)
   - Tipo (venta/alquiler)
   - Precio min/max
   - Habitaciones, mÂ², etc.
3. **Copia la URL completa** de la pÃ¡gina de resultados

**Ejemplo de URL**:
```
https://www.idealista.com/venta-viviendas/madrid/chamberi/con-precio-hasta_300000,precio-desde_200000/
```

### Paso 2: Configurar Google Sheets

(Igual que antes - necesitas `credentials.json`)

### Paso 3: Instalar Dependencias

```bash
cd /Users/ericruiz/Desktop/Hogar
pip install -r requirements.txt
```

Nuevas dependencias aÃ±adidas:
- `beautifulsoup4` - Parser HTML
- `lxml` - Parser rÃ¡pido
- `selenium` - Para sitios con JavaScript (opcional)

### Paso 4: Ejecutar el Tracker

```bash
# Entrecomilla la URL porque puede tener caracteres especiales
python scrape_tracker.py "TU_URL_DE_BUSQUEDA_AQUI"
```

**Ejemplo real**:
```bash
python scrape_tracker.py "https://www.idealista.com/venta-viviendas/madrid/chamberi/con-precio-hasta_300000/"
```

### Paso 5: Â¡Listo!

El script:
- âœ… Scrapea la pÃ¡gina de bÃºsqueda cada 30 min (configurable en `.env`)
- âœ… Detecta propiedades nuevas automÃ¡ticamente
- âœ… Las aÃ±ade a Google Sheets
- âœ… Evita duplicados
- âœ… Corre indefinidamente hasta que lo detengas (Ctrl+C)

## ğŸ¯ Casos de Uso

### Caso 1: Monitorear Pisos en tu Barrio

```bash
# 1. Ve a Idealista y filtra:
#    - Tu barrio
#    - Precio mÃ¡ximo
#    - MÃ­nimo de habitaciones

# 2. Copia la URL

# 3. Ejecuta:
python scrape_tracker.py "URL_COPIADA"

# Â¡Ya estÃ¡s monitoreando! ğŸ‰
```

### Caso 2: AÃ±adir Propiedades Favoritas

```bash
# 1. Crea urls.txt con las URLs que te interesan
# 2. Ejecuta:
python scrape_urls.py urls.txt

# Todas se aÃ±aden a Google Sheets
```

### Caso 3: BÃºsqueda Ãšnica Diaria

```bash
# AÃ±ade a cron para ejecutar cada dÃ­a:
0 9 * * * cd /Users/ericruiz/Desktop/Hogar && python scrape_tracker.py "URL" once
```

## ğŸ“Š Datos ExtraÃ­dos

El scraper extrae los mismos 20 campos que con API:

- âœ… ID de la propiedad
- âœ… TÃ­tulo
- âœ… Precio
- âœ… TamaÃ±o (mÂ²)
- âœ… Precio/mÂ²
- âœ… Habitaciones
- âœ… BaÃ±os
- âœ… Planta
- âœ… Exterior (sÃ­/no)
- âœ… Ascensor (sÃ­/no)
- âœ… Parking (sÃ­/no)
- âœ… DirecciÃ³n completa
- âœ… Distrito
- âœ… Municipio
- âœ… Provincia
- âœ… URL del anuncio
- âœ… Imagen principal
- âœ… DescripciÃ³n
- âœ… Fecha de detecciÃ³n
- âœ… Estado

## âš™ï¸ ConfiguraciÃ³n

### Cambiar Intervalo de BÃºsqueda

Edita `.env`:
```env
CHECK_INTERVAL_MINUTES=30  # Por defecto 30 minutos
```

**Recomendaciones**:
- **15-30 min**: Ã“ptimo para no perder propiedades
- **60 min**: Si la zona no es muy activa
- **5-10 min**: Solo si es MUY urgente (no recomendado)

### Delay entre Peticiones

En `idealista_scraper.py`, mÃ©todo `delay_between_requests()`:
```python
scraper.delay_between_requests(2)  # 2 segundos por defecto
```

## ğŸš¨ SoluciÃ³n de Problemas

### "Error 403 - Forbidden"

Idealista detectÃ³ demasiadas peticiones:
- **SoluciÃ³n**: Aumenta el delay entre peticiones
- Espera 5-10 minutos antes de reintentar
- Reduce la frecuencia de bÃºsqueda

### No Extrae Algunos Datos

Idealista cambiÃ³ el HTML:
- **SoluciÃ³n**: Abre un issue o actualiza los selectores en `idealista_scraper.py`
- Usa el navegador para inspeccionar las nuevas clases CSS

### "No se encontraron propiedades"

Posibles causas:
1. La URL es incorrecta â†’ Copia de nuevo desde el navegador
2. Idealista cambiÃ³ la estructura â†’ Actualizar selectores
3. Filtros muy restrictivos â†’ AmplÃ­a la bÃºsqueda

### Scraping Muy Lento

Si necesitas velocidad:
- Los datos del listado son suficientes (no hace falta scrapear cada URL individual)
- El modo actual scrapea solo la pÃ¡gina de bÃºsqueda (rÃ¡pido)
- Comentado: scraping de detalles completos (lento pero mÃ¡s datos)

## ğŸ“ ComparaciÃ³n: API vs Scraping

| CaracterÃ­stica | Scraping ğŸ•·ï¸ | API ğŸ”Œ |
|---------------|-------------|--------|
| **Coste** | GRATIS | Gratis/Pago |
| **Setup** | Inmediato | DÃ­as (aprobaciÃ³n) |
| **Credenciales** | No necesita | Requiere |
| **LÃ­mites** | Solo tasa respeto | Cuotas por plan |
| **Datos** | Todos visibles | Limitados por API |
| **Mantenimiento** | Medio (cambios HTML) | Bajo (estable) |
| **Velocidad** | Media | Alta |
| **Legalidad** | Zona gris* | Autorizado |
| **Recomendado** | Uso personal | Uso comercial |

\* Verifica los tÃ©rminos de servicio

## ğŸ’¡ Tips y Trucos

### 1. BÃºsquedas Guardadas

Guarda tus URLs favoritas en un archivo:

```bash
# urls_favoritas.sh
python scrape_tracker.py "URL_MADRID_CENTRO" once
python scrape_tracker.py "URL_BARCELONA" once
python scrape_tracker.py "URL_VALENCIA" once
```

### 2. MÃºltiples Zonas

Ejecuta mÃºltiples instancias simultÃ¡neamente:

```bash
# Terminal 1
python scrape_tracker.py "URL_ZONA_1"

# Terminal 2
python scrape_tracker.py "URL_ZONA_2"

# Terminal 3
python scrape_tracker.py "URL_ZONA_3"
```

### 3. Exportar Datos

Desde Google Sheets:
- Archivo â†’ Descargar â†’ CSV/Excel
- Analiza con Python, Excel, Tableau, etc.

### 4. Alertas Personalizadas

Modifica `scrape_tracker.py` para aÃ±adir condiciones:

```python
# Solo aÃ±adir si el precio/mÂ² es bueno
if prop['precio_m2'] < 3000:
    sheets.add_property(prop)
    # Enviar alerta urgente
```

### 5. Scraping HistÃ³rico

Para obtener datos del pasado:
- Scrapea pÃ¡ginas 1, 2, 3... de resultados
- Modifica la URL: aÃ±ade `?pagina=2`, `?pagina=3`, etc.

## ğŸ”’ Ã‰tica y Legalidad

### âœ… Buenas PrÃ¡cticas

1. **Respeta el servidor**: Delays de 2+ segundos
2. **Uso personal**: No revender los datos
3. **No saturar**: Evita demasiadas peticiones simultÃ¡neas
4. **IdentificaciÃ³n**: User-Agent honesto (ya incluido)
5. **Robots.txt**: Respeta las reglas del sitio

### âš–ï¸ Legalidad

- **EspaÃ±a**: El scraping de datos pÃºblicos suele ser legal
- **GDPR**: No scrapees datos personales sensibles
- **TÃ©rminos de servicio**: Idealista puede prohibirlo en sus ToS
- **Uso**: Personal OK, comercial consulta abogado

**Disclaimer**: Esto no es asesoramiento legal. Consulta con un abogado para casos comerciales.

## ğŸš€ Siguiente Nivel

### Scraping con JavaScript

Si Idealista usa mucho JS:

```bash
pip install selenium
```

Luego modifica el scraper para usar Selenium (mÃ¡s lento pero mÃ¡s completo)

### Proxies Rotantes

Para scraping intensivo:

```python
proxies = {
    'http': 'http://proxy1.com:8080',
    'https': 'https://proxy1.com:8080',
}
response = requests.get(url, proxies=proxies)
```

### OCR para Captchas

Si aparecen captchas:
- Usa servicios como 2captcha
- O reduce la frecuencia

## ğŸ“š Recursos

- **BeautifulSoup Docs**: https://www.crummy.com/software/BeautifulSoup/bs4/doc/
- **Requests Docs**: https://requests.readthedocs.io/
- **Web Scraping Legal Guide**: https://blog.apify.com/is-web-scraping-legal/

---

## âœ… Checklist de Scraping

- [ ] Dependencias instaladas (`pip install -r requirements.txt`)
- [ ] Google Sheets configurado
- [ ] URL de bÃºsqueda copiada desde Idealista
- [ ] Intervalo configurado en `.env`
- [ ] Primera ejecuciÃ³n: `python scrape_tracker.py "URL" once`
- [ ] Todo OK â†’ Ejecutar en continuo: `python scrape_tracker.py "URL"`

---

**Â¡Con scraping puedes empezar a trackear propiedades SIN ninguna API en menos de 5 minutos! ğŸ•·ï¸âœ¨**


